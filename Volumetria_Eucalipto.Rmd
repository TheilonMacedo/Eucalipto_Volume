---
title: "Volumetria de Eucalipto usando o tidymodels"
author: "Theilon Macedo"
date: "29/08/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) 
```

O dataset utilizado neste projeto é oriundo do trabalho de [Azevedo et al. (2020)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0238703).

Umas das dificuldades encontradas na academia é a reproducibilidade de trabalhos. Essa é uma situação que, infelizmente, é corriqueira na área florestal. Recentemente, alguns esforços tem sido feitos para melhorar a forma de que trabalhos podem ser replicados, das quais descreverei neste post. Aproveitando o embalo, também vou criar mostrar como utilizar o pacote tidymodels para a predição do volume de eucalipto. O dataset aqui utilizado neste projeto é oriundo do excelente trabalho publicado no ano de 2020 pelo profesor [Gileno Azevedo juntamente com outros pesquisadores.](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0238703) O dataset pode ser encontrado na página do artigo.
  
## 1ª etapa - Iniciando o projeto  

Antes de qualquer coisa, preciso falar de 3 ferramentas básicas quando se busca reproducibilidade em trabalhos no R:   

* renv  
Este pacote é um gerenciador de dependências: ele organiza e "memoriza" as dependências (como pacotes) que o seu projeto está usando de modo que caso alguém vá refazer suas análises não ocorra problemas como o uso de pacotes de versões distintas, além de tornar seu ambiente de trabalho mais isolado. Mais à frente explicarei outra importante vantagem. Caso queira saber mais sobre o renv, esse tutorial do [Chaoran](https://6chaoran.wordpress.com/2020/07/20/introduction-of-renv-package/) é show.
* R Projects  
É o mais simples dessa lista. Basicamente toda vez que você for criar um projeto, o ideal é que seja criada uma pasta principal e dentro dela serão criadas as demais pastas (data, scripts, etc.). Em seguida, abra o RStudio e crie um Project dentro desta pasta principal. Mais ou menos com a seguinte estrutura utilizada neste trabalho:  


```{r eval=FALSE}
# Eucalipto_Volume (pasta principal)  
#      |--Eucalipto_Volume (R Project) 
#      |--R  
#      |--paper
#      |--plots
#      |--renv
#      |--report
```
   
* here  
Esse aqui é o mais legal: basicamente ele te permite criar caminhos relativos. Por ex.: ao invés de se referir a um arquivo como `dados <- C:user/zezin/uma/duas/dados/meus_dados.csv`, ou usar `setwd("C:user/zezin/uma/duas/dados")`, prefira salvar seu Project em uma pasta principal e se referir a ele como `here::here("dados", "meus_dados.csv")`. Isso facilita que outras pessoas reproduza suas análises sem grandes problemas. Tem um pessoal que recomenda [fortemente](https://twitter.com/jennybryan/status/940436177219338240) usar o pacote here. Basicamente ele seta o caminho de acesso aos arquivos a partir da última pasta do caminho (top-level foder), que no nosso exemplo hipotético seria a pasta "dados".  

### Mão na massa

A primeira coisa que fiz foi criar uma pasta principal. Em seguida, criei dentro dela um R Projects com o nome do meu projeto. Então instalei o renv e executei o comando renv::init() para criar um ambiente local do projeto. A partir daí, iniciei as análises e instalei os pacotes básicos iniciais necessários (incluindo o {here}). Sempre que instalava um novo pacote, chamava renv::snapshot() para atualizar os estado do projeto. Caso fosse necessário restaurar o projeto após chamar renv::snapshot(), era acionado o comando renv::restore(). Pronto! Meu ambiente local do projeto estámomtado e isolado. Bacana, né?  


## 2ª etapa - Modelagem
Agora a parte legal: a modelagem dos dados. Recentemente tem ganhado bastante tração um pacote de modelagem do R baseado na filosofia do tidyverse: o [tidymodels](https://www.tidymodels.org/). Esse pacote é incrível, permitindo usar diversos mdoelos de machine learning de forma extremamente intuitiva, além de bem elegante. A seguir vou mostrar o passo a passo necessário para ajustar e testar diversos modelos visando a predição do volume de eucalipto. Vamo lá!


Carregando os pacotes necessários:

```{r eval = FALSE}
library(here)
library(tidyverse)
library(tidymodels)
library(janitor)
library(EnvStats)
library(bestNormalize)
library(finetune)
library(doParallel)
library(extrafont)

```

### Aquisição dos dados  

Primeiramente serão adquiridas as bases de dados (treino e teste) a serem utilizadas no projeto a partir do site que disponibiliza o artigo e os dados. Aqui será criada uma pasta "data" onde os arquivos da base de dados serão baixados.  

```{r eval = FALSE}

# Criando o diretório onde a base será alocada
dir.create("data")

# Acesso ao material
link_train <- "https://doi.org/10.1371/journal.pone.0238703.s007"
link_test <- "https://doi.org/10.1371/journal.pone.0238703.s008"

# Aqui é criado o caminho de acesso às bases de dados
dest_folder_train <- here::here("data", "train.xlsx")
dest_folder_test <- here::here("data", "test.xlsx")

# Aquisição da base de teste e treino usadas pelos autores do trabalho
utils::download.file(link_train, destfile = dest_folder_train, mode = "wb")
utils::download.file(link_test, destfile = dest_folder_test, mode = "wb")

```

Perceba que aqui é usado o pacote {here} para a criação de um caminho relativo até a pasta de destino, em que o caminho até a pasta obejetivo (neste caso, a pasta "data") se inicia a partir da pasta principal criada para o projeto, e não do seu diretório base. Caso a pasta principal do projeto seja mudada para outra partição, não será necessário chamar `setwd()` com um novo caminho.  
E qual a vantagem disso? Isso permite que você ou qualquer outra pessoa não precise alterar o caminho caso queira carregar/salvar as informações obtidas das mesmas análises aqui apresentadas. A utilização desse pacote é uma boa prática quando falamos de project-oriented workflows (workflow montado tendo como base projects).  

### Preparando os dados  
As bases de dados foram separadas originalmente pelos autores em dois arquivos. Essa é uma boa prática para criação de modelos e sua subsequente avaliação. Dividir os sets dessa forma permite que não ocorra data leakage (quando seu modelo, na fase de treino, "entra em contato" com os dados de teste, causando overfitting). Entretanto, uma vantagem do tidymodels é que ele permite o split entre set de treino e set de teste de forma unificada, sendo geradas partições de um mesmo arquivo. Para ilustrar essa capacidade, os datasets serão unidos e então será mostrado como realizar a sua separação em treino e teste com o tidymodels:

```{r eval = FALSE}
# Carregando a base de daoos separada
sheet_train <- readxl::read_excel(here::here("data", "train.xlsx"), skip = 1)
sheet_test <- readxl::read_excel(here::here("data", "test.xlsx"), skip = 1)

# Unificando a base e "limpando os nomes" das colunas
dataset <- 
    dplyr::bind_rows(sheet_train, sheet_test) |>
    janitor::clean_names() |> 
    dplyr::mutate(across(c(stem:rotation), forcats::as_factor),
                  across(c(tx, d), as.integer))

```

### Iniciando o processo de modelagem
Após unificar os dados, vamo colocar a mão na massa!
A primeira coisa a se fazer é dividir os dados em treino e teste e então realizar a reamostragem repetida do set de treino para validação dos modelos. O tidymodels faz isso de forma prática de modo que não acontece data leakage:

```{r eval = FALSE}

set.seed(1504)

vol_split <- rsample::initial_split(data = dataset, prop = 0.75, strata = cod_volume)
train_split <- rsample::training(vol_split)
test_split <- rsample::testing(vol_split)

# Realizando validação cruzada repetida para avaliação dos modelos
vol_folds <- rsample::vfold_cv(data = train_split, strata = cod_volume, repeats = 5)

```

Algumas considerações nessa etapa:  

* O split inicial foi realizado usando uma proporção de 3/4.  
* A divisão foi estratificada de acordo com as classes de volume.  
* O tidymodels permite a divisão do dataset de forma simples e que não ocorra data leakage, dispensando a necessidade de separar os dados manualmente.

### Explorando os dados
Neste caso, temos um set de dados com poucos preditores: dbh (DAP) e h (Altura). Neste dataset também consta as variáveis tx e d que dizem respeito a se o volume de madeira predito é com casca ou sem casca (0 ou 1) e o diâmetro comercial. Realizando uma análise exploratória básica das variáveis:

```{r eval=FALSE}

ggplot(data = train_split) +
    geom_density(mapping = aes(x = dbh)) +
    labs(x = "DAP")

ggplot(data = train_split) +
    geom_density(mapping = aes(x = h)) +
    labs(x = "Altura")

# Avaliando o impacto da normalização dos dados
(penalty <- bestNormalize::boxcox(train_split$h))

train_split |> 
    mutate(bc = EnvStats::boxcoxTransform(train_split$h, lambda = penalty$lambda)) |>  
    ggplot(mapping = aes(x = bc)) +
    geom_density() +
    labs(x = "Altura transformada")

# Explorando as relações entre as variáveis principais
ggplot(data = train_split) +
    stat_count(mapping = aes(x = cod_volume))

ggplot(data = train_split) +
    geom_point(mapping = aes(x = dbh, y = v))

ggplot(data = train_split) +
    geom_point(mapping = aes(x = dbh^2, y = v))

ggplot(data = train_split) +
    geom_point(mapping = aes(x = h^2, y = v))

ggplot(data = train_split) +
    geom_point(mapping = aes(x = h, y = v))

ggplot(data = train_split) +
    geom_point(mapping = aes(x = dbh*h, y = v))

ggplot(data = train_split) +
    geom_point(mapping = aes(x = (dbh*h)^2, y = v))

ggplot(data = train_split) +
    geom_point(mapping = aes(x = dbh, y = h))

```


Pôde-se perceber que a variável altura possui uma distribuição assimétrica à esquerda. Neste caso, optou-se por avaliar se a transformação de Box-Cox poderia tonar a distribuição das variáveis mais "Gaussiana" (Normal). Esta transformação parece ter tido um efeito desejável sobre a distribuição das alturas. Além disso, é recomendável padronizar os dados (center and scale) antes de seguir com a modelagem. Sem a padronização, uma variável pode ter um maior [impacto](https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia) sobre a resposta apenas por conta da sua escala, o que pode ser o caso aqui dadas as própria unidades (cm e m) dos dados e a transformação aplicada. Essa etapa será aplicada mais à frente na modelagem.  
No último plot, é opssível observar que as classes de volume não possuem a mesma quantidade de observações. Neste caso, pode-se seguir com duas estratégias: a primeira seria ajustar um modelo para cada um dos diferentes volumes em separado; a segunda consiste na divisão ponderada (estratificação) no split dos dados (o treino e o teste teriam a mesma proporção de cada classe de volume). Neste trabalho, seguiremos a segunda estratégia.


### Ajustando os modelos
Agora vem a parte legal e onde a mágica do tidymodels acontece:
Primeiro é definida uma recipe do passo-a-passo que os dados devem ser processados. Em seguida são definidas as especificações modelos a serem utilizados. Serão definidas duas recipes (receitas de passo-a-passo): uma sem pré-processamento e outra pré-processada:

```{r eval=FALSE}
# Definindo os preprocessamentos dos dados
simple_vol_rec <- recipes::recipe(v ~ dbh + h + tx + d, data = train_split)

normalized_vol_rec <- simple_vol_rec |> 
  recipes::step_interact(terms = ~ dbh:h) |>  
  recipes::step_mutate(dbh_sqrd = dbh ^ 2, h_sqrd = h ^ 2) |> 
  recipes::step_normalize(recipes::all_predictors(), -tx, -d)

```

Aqui pode-se perceber que a recipe `normalized_vol_rec` é apenas a recipe `simple_vol_rec` com mais "passos"
adicionados.  

Em seguida, são definidos os diversos modelos a serem ajustados e depois testados. Estes modelos terão os hiperparâmetros mais importantes "marcados" para tunagem usando a função tune do pacote {tune}. 

```{r eval=FALSE}
# Definindo diversos modelos
lm_mod <- 
     parsnip::linear_reg() |> 
     parsnip::set_engine("lm") |> 
     parsnip::set_mode("regression")
  
penalized_lm_mod <- 
  parsnip::linear_reg(penalty = tune::tune(),
                      mixture = tune::tune()) |> 
  parsnip::set_engine("glmnet") |>
  parsnip::set_mode("regression")

bag_mars_mod <- 
  baguette::bag_mars(prod_degree = tune::tune(), 
                     prune_method = "exhaustive",
                     num_terms = tune::tune()) |> 
  parsnip::set_engine("earth", times = 4) |> 
  parsnip::set_mode("regression")
  
dec_tree <- 
  parsnip::decision_tree(cost_complexity = tune::tune(),
                         tree_depth = tune::tune(),
                         min_n = tune::tune()) |> 
  parsnip::set_engine("rpart") |> 
  parsnip::set_mode("regression")

bag_cart_mod <- 
  baguette::bag_tree(cost_complexity = tune::tune(),
                     tree_depth = tune::tune(),
                     min_n = tune::tune()) |> 
  parsnip::set_engine("rpart", times = 50L) |>
  parsnip::set_mode("regression")

rf_spec <- 
  parsnip::rand_forest(mtry = tune::tune(), 
                       min_n = tune::tune(),
                       trees = 1000) |> 
  parsnip::set_engine("ranger") |> 
  parsnip::set_mode("regression")
  
xgb_spec <- 
  parsnip::boost_tree(tree_depth = tune::tune(),
                      learn_rate = tune::tune(),
                      loss_reduction = tune::tune(),
                      min_n = tune::tune(),
                      sample_size = tune::tune(),
                      mtry = tune::tune(),
                      trees = 1000,
                      stop_iter = 20) |> 
  parsnip::set_engine("xgboost") |> 
  parsnip::set_mode("regression")
  
svm_r_spec <- 
  parsnip::svm_rbf(cost = tune::tune(),
                   rbf_sigma = tune::tune(),
                   margin = tune::tune()) |> 
    parsnip::set_engine("kernlab") |> 
    parsnip::set_mode("regression")
  
nnet_spec <- 
  parsnip::mlp(hidden_units = tune::tune(),
               penalty = tune::tune(), 
               epochs = tune::tune()) |> 
    parsnip::set_engine("nnet") |> 
    parsnip::set_mode("regression")
  
  
specs_vol <- list("linear_reg" = lm_mod, "bag_mars"= bag_mars_mod,
                  "decision_tree" = dec_tree, "bag_cart" = bag_cart_mod, 
                  "rf" = rf_spec, "xgb" = xgb_spec,
                  "svm_rbf" = svm_r_spec, "nnet_mlp" = nnet_spec,
                  "penalized_reg" = penalized_lm_mod)


```

Todas as especificações foram alocadas em uma lista que será usada mais à frente. Até aqui temos as duas recipes de pré-processamento a serem usadas nos dados e os 9 modelos especificados a serem tunados/ajustados.  

Agora vem a pergunta: como juntar tudo isso?  
Em um workflow (ou um workflowset, melhor dizendo)  

```{r eval=FALSE}

# Definindo o workflowset (serão ajustados 18 modelos ao todo)
wflow_vol <- workflowsets::workflow_set(
  models = specs_vol, #lista de modelos
  preproc = list(
    normalized = normalized_vol_rec,
    simple = simple_vol_rec)
  )


```

Em seguida será definida a configuração e o tipo de tunagem. Neste caso, o processo de tunagem avalia todos os modelos em um set inicial da reamostragem. Baseado na perforance corrente das métricas, alguns parâmetros são então descartados no prosseguimento do processo. Sendo realizada uma análise preliminar para a remoção de parâmetros que produzam modelos de baixa qualiade. Mais no livro do [tidymodels](https://www.tmwr.org/grid-search.html#racing).

```{r eval=FALSE}
# Definindo as configurações da tunagem de parâmetros usando computação paralela
racing_ctrl <- finetune::control_race(
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)


# Definindo a execução em paralelo 
clusters <- parallel::detectCores() - 5
cl <- parallel::makePSOCKcluster(clusters)
doParallel::registerDoParallel(cl)

# Realizando a tunagem dos modelos
results_vol <-
  workflowsets::workflow_map(
    wflow_vol,
    seed = 1504,
    resamples = vol_folds,
    control = racing_ctrl,
    fn = "tune_race_anova",
    grid = 25,
    metrics = yardstick::metric_set(yardstick::rmse,
                                    yardstick::rsq, 
                                    yardstick::huber_loss, 
                                    yardstick::mae)
)
# Parando a execução em paralelo 
parallel::stopCluster(cl)
foreach::registerDoSEQ()

```

Após realização da tunagem, é feita a avaliação dos modelos de acordo com gráficos de comparação:

```{r eval=FALSE}
# Plotagem dos resultados para cada métrica utilizada
plot_results <- function(race_rslts, mtrc = "rmse",...){
  workflowsets::autoplot(
    race_rslts,
    rank_metric = mtrc,  
    metric = mtrc,       
    select_best = TRUE,
    ...
    ) -> plot_racing
  
  ggplot2::ggsave(glue::glue(mtrc, ".png"), path = here::here("plots"))
  graphics::plot(plot_racing)
  
}

plot_results(results_vol, mtrc = "rsq")
plot_results(results_vol, mtrc = "rmse")
plot_results(results_vol, mtrc = "huber_loss")
plot_results(results_vol, mtrc = "mae")

```

### Avaliação da qualidade dos modelos

Então são definidas funções para:  

* seleção dos modelos
* ajuste do modelo selecionado  
* obtenção das métricas de avaliação do modelo no set de teste.

```{r eval=FALSE}
# Função para rankeamento de modelos de acordo com o R²
select_models <- function(grid_results, metric = "rsq", rank_posit = 1){
  
  workflowsets::rank_results(grid_results, select_best = TRUE) |> 
  dplyr::relocate(rank) |> 
  dplyr::select(-c(.config, model, std_err)) |> 
  dplyr::filter(.metric == "rsq" & rank == rank_posit) -> model_selected
  EnvStats::print(model_selected)
  
}

# Selecionando o modelo e realizando o seu ajuste final
fit_model <- function(grid_results, model_ranked, df_split, metric = "rmse"){
    
    name_model <- model_ranked |> purrr::pluck("wflow_id", 1)
  
    model <- grid_results |> 
        workflowsets::extract_workflow_set_result(id = name_model) |>
        tune::select_best(metric = metric)
    
    grid_results |> 
        workflowsets::extract_workflow(name_model) |>
        tune::finalize_workflow(model) |> 
        tune::last_fit(split = df_split,
                       metrics = yardstick::metric_set(yardstick::rmse, 
                                                       yardstick::rsq, 
                                                       yardstick::huber_loss, 
                                                       yardstick::mae))
    
}

# Obtendo a performance do melhor modelo no set de teste
metrics_mod <- function(best_mod, model_ranked){
  
  name_model <- model_ranked |> purrr::pluck("wflow_id", 1)
  
  workflowsets::collect_metrics(best_mod) |> 
    kableExtra::kbl(caption = glue::glue("Performance do modelo ", name_model)) |> 
    kableExtra::kable_classic(full_width = F, 
                              html_font = "Cambria", 
                              font_size = 16) |> 
    kableExtra::save_kable(file = here::here("plots", glue::glue("perf_", name_model, ".png")),
                           self_contained = T)
  
  workflowsets::collect_metrics(best_mod)
}

# Seleção, ajuste e avaliação do melhor modelo
best_model <- select_models(grid_results = results_vol)

fit_best_mod <- fit_model(grid_results = results_vol,
                        model_ranked = best_model,
                        df_split = vol_split)

metrics_mod(fit_best_mod, best_model)

```

De forma didática, será comparado um modelo de qualidade intermediárias dentre todos os que foram ajustados. Esse processo se dá de forma similar ao realizado previamente, exceto que agora será selecionado o modelo rankeado na posição número 10 em contraste ao melhor modelo (rank 1):  

```{r eval=FALSE}
# Avaliando um modelo intermediário
intermediary_model <- select_models(grid_results = results_vol, rank_posit = 10)

# Obtendo a performance de um modelo intermediário no set de teste
fit_intermediary_model <- fit_model(grid_results = results_vol,
                                       model_ranked = intermediary_model,
                                       df_split = vol_split)

metrics_mod(fit_intermediary_model, intermediary_model)

```

Agora ambos os modelos ajustados previamente serão comparados através de gráficos de dispersão de volume predito e volume observado, permitindo uma análise visual da qualidade preditiva de cada um. Para isso serão definidas duas funções para geração do gráfico de dispersão de cada modelo.

```{r eval=FALSE}
# Função para plotar os resultados dos modelos
scatterplot <- function(.x, .y){
    
    ggplot2::ggplot(.x) + 
        ggplot2::geom_point(mapping = ggplot2::aes(x = .pred, y = v)) + 
        ggplot2::geom_point(mapping = ggplot2::aes(x = .pred, y = v), 
                   alpha = 0.5, 
                   size = 2, 
                   color = "black", 
                   fill = "mediumspringgreen", 
                   pch = 21) + 
        ggplot2::geom_abline(lty = 2, 
                    col = "black", 
                    size = 1) +
        tune::coord_obs_pred() +
        ggplot2::theme_classic() +
        ggplot2::theme(text = ggplot2::element_text(family = "Source Code Pro")) +
        ggplot2::ggtitle(.y) +
        ggplot2::labs(x = "Volume Predito (m³)",
             y = "Volume Observado (m³)") -> plot_scatterplot
    
  ggplot2::ggsave(glue::glue(.y, ".png"), path = here::here("plots"))
  graphics::plot(plot_scatterplot)
  
}

# Função para extrair os dados preditos e plotá-los versus os dados observados usando a função anterior
accessing_models <- function(mod1, mod2, model_ranked1, model_ranked2){
    
    gsub_und <- function(x) gsub("_", " ", x)
    name_model1 <- model_ranked1 |> purrr::pluck("wflow_id", 1) |> gsub_und()
    name_model2 <- model_ranked2 |> purrr::pluck("wflow_id", 1) |> gsub_und()
    
    workflowsets::collect_predictions(mod1) |> 
        dplyr::mutate(model = name_model1) |> 
        dplyr::bind_rows(workflowsets::collect_predictions(mod2) |> 
                      dplyr::mutate(model = name_model2)) |>
        tidyr::nest(data = -c(model)) |> 
        dplyr::mutate(plots = map2(data, model, scatterplot))
    
}

accessing_models(fit_best_mod, fit_intermediary_model, best_model, intermediary_model)
```

### Conclusão da modelagem
É possível observar que o modelo XGBoost apresentou um desempenho elevado frente aos demais modelos,
apontando a sua aplicabilidade para a predição de volume de madeira de eucalipto em florestas platandas. É digno de nota que, de forma similar ao que foi apontado por Azevedo et al. (2020), o modelo de redes neurais artificiais (ANN) apresentou um R² de ~0,96.  

## 3ª etapa - Reproducibilidade